# YOLOv5 ğŸš€ by Ultralytics, AGPL-3.0 license
"""
PyTorch utils
"""

import math
import os
import platform
import subprocess
import time
import warnings
from contextlib import contextmanager
from copy import deepcopy
from pathlib import Path

# ä»¥ä¸‹æ˜¯ä¸€äº›åŸºæœ¬çš„torchç›¸å…³çš„ç±»
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parallel import DistributedDataParallel as DDP

from utils.general import LOGGER, check_version, colorstr, file_date, git_describe

LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html
RANK = int(os.getenv('RANK', -1))
WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))

try:
    import thop  # for FLOPs computation
except ImportError:
    thop = None

# Suppress PyTorch warnings
warnings.filterwarnings('ignore', message='User provided device_type of \'cuda\', but CUDA is not available. Disabling')
warnings.filterwarnings('ignore', category=UserWarning)


def smart_inference_mode(torch_1_9=check_version(torch.__version__, '1.9.0')):
    # Applies torch.inference_mode() decorator if torch>=1.9.0 else torch.no_grad() decorator
    def decorate(fn):
        return (torch.inference_mode if torch_1_9 else torch.no_grad)()(fn)

    return decorate


def smartCrossEntropyLoss(label_smoothing=0.0):
    # Returns nn.CrossEntropyLoss with label smoothing enabled for torch>=1.10.0
    if check_version(torch.__version__, '1.10.0'):
        return nn.CrossEntropyLoss(label_smoothing=label_smoothing)
    if label_smoothing > 0:
        LOGGER.warning(f'WARNING âš ï¸ label smoothing {label_smoothing} requires torch>=1.10.0')
    return nn.CrossEntropyLoss()


def smart_DDP(model):
    # Model DDP creation with checks
    assert not check_version(torch.__version__, '1.12.0', pinned=True), \
        'torch==1.12.0 torchvision==0.13.0 DDP training is not supported due to a known issue. ' \
        'Please upgrade or downgrade torch to use DDP. See https://github.com/ultralytics/yolov5/issues/8395'
    if check_version(torch.__version__, '1.11.0'):
        return DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK, static_graph=True)
    else:
        return DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK)


def reshape_classifier_output(model, n=1000):
    # Update a TorchVision classification model to class count 'n' if required
    from models.common import Classify
    name, m = list((model.model if hasattr(model, 'model') else model).named_children())[-1]  # last module
    if isinstance(m, Classify):  # YOLOv5 Classify() head
        if m.linear.out_features != n:
            m.linear = nn.Linear(m.linear.in_features, n)
    elif isinstance(m, nn.Linear):  # ResNet, EfficientNet
        if m.out_features != n:
            setattr(model, name, nn.Linear(m.in_features, n))
    elif isinstance(m, nn.Sequential):
        types = [type(x) for x in m]
        if nn.Linear in types:
            i = types.index(nn.Linear)  # nn.Linear index
            if m[i].out_features != n:
                m[i] = nn.Linear(m[i].in_features, n)
        elif nn.Conv2d in types:
            i = types.index(nn.Conv2d)  # nn.Conv2d index
            if m[i].out_channels != n:
                m[i] = nn.Conv2d(m[i].in_channels, n, m[i].kernel_size, m[i].stride, bias=m[i].bias is not None)


@contextmanager
def torch_distributed_zero_first(local_rank: int):
    """ç”¨åœ¨train.py
    ç”¨äºå¤„ç†æ¨¡å‹è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒæ—¶åŒæ­¥é—®é¢˜
    åŸºäºtorch.distributed.barrier()å‡½æ•°çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œä¸ºäº†å®Œæˆæ•°æ®çš„æ­£å¸¸åŒæ­¥æ“ä½œï¼ˆyolov5ä¸­æ‹¥æœ‰å¤§é‡çš„å¤šçº¿ç¨‹å¹¶è¡Œæ“ä½œï¼‰
    Decorator to make all processes in distributed training wait for each local_master to do something.
    :params local_rank: ä»£è¡¨å½“å‰è¿›ç¨‹å·  0ä»£è¡¨ä¸»è¿›ç¨‹  1ã€2ã€3ä»£è¡¨å­è¿›ç¨‹
    """
    if local_rank not in [-1, 0]:
        # å¦‚æœæ‰§è¡Œcreate_dataloader()å‡½æ•°çš„è¿›ç¨‹ä¸æ˜¯ä¸»è¿›ç¨‹ï¼Œå³rankä¸ç­‰äº0æˆ–è€…-1ï¼Œ
        # ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¼šæ‰§è¡Œç›¸åº”çš„torch.distributed.barrier()ï¼Œè®¾ç½®ä¸€ä¸ªé˜»å¡æ …æ ï¼Œ
        # è®©æ­¤è¿›ç¨‹å¤„äºç­‰å¾…çŠ¶æ€ï¼Œç­‰å¾…æ‰€æœ‰è¿›ç¨‹åˆ°è¾¾æ …æ å¤„ï¼ˆåŒ…æ‹¬ä¸»è¿›ç¨‹æ•°æ®å¤„ç†å®Œæ¯•ï¼‰ï¼›
        dist.barrier(device_ids=[local_rank])
    yield  # yieldè¯­å¥ ä¸­æ–­åæ‰§è¡Œä¸Šä¸‹æ–‡ä»£ç ï¼Œç„¶åè¿”å›åˆ°æ­¤å¤„ç»§ç»­å¾€ä¸‹æ‰§è¡Œ
    if local_rank == 0:
        # å¦‚æœæ‰§è¡Œcreate_dataloader()å‡½æ•°çš„è¿›ç¨‹æ˜¯ä¸»è¿›ç¨‹ï¼Œå…¶ä¼šç›´æ¥å»è¯»å–æ•°æ®å¹¶å¤„ç†ï¼Œ
        # ç„¶åå…¶å¤„ç†ç»“æŸä¹‹åä¼šæ¥ç€é‡åˆ°torch.distributed.barrier()ï¼Œ
        # æ­¤æ—¶ï¼Œæ‰€æœ‰è¿›ç¨‹éƒ½åˆ°è¾¾äº†å½“å‰çš„æ …æ å¤„ï¼Œè¿™æ ·æ‰€æœ‰è¿›ç¨‹å°±è¾¾åˆ°äº†åŒæ­¥ï¼Œå¹¶åŒæ—¶å¾—åˆ°é‡Šæ”¾ã€‚
        dist.barrier(device_ids=[0])


def device_count():
    # Returns number of CUDA devices available. Safe version of torch.cuda.device_count(). Supports Linux and Windows
    assert platform.system() in ('Linux', 'Windows'), 'device_count() only supported on Linux or Windows'
    try:
        cmd = 'nvidia-smi -L | wc -l' if platform.system() == 'Linux' else 'nvidia-smi -L | find /c /v ""'  # Windows
        return int(subprocess.run(cmd, shell=True, capture_output=True, check=True).stdout.decode().split()[-1])
    except Exception:
        return 0


def select_device(device='', batch_size=0, newline=True):
    """å¹¿æ³›ç”¨äºtrain.pyã€val.pyã€detect.pyç­‰æ–‡ä»¶ä¸­
    ç”¨äºé€‰æ‹©æ¨¡å‹è®­ç»ƒçš„è®¾å¤‡ å¹¶è¾“å‡ºæ—¥å¿—ä¿¡æ¯
    :params device: è¾“å…¥çš„è®¾å¤‡  device = 'cpu' or '0' or '0,1,2,3'
    :params batch_size: ä¸€ä¸ªæ‰¹æ¬¡çš„å›¾ç‰‡ä¸ªæ•°
    """
    # git_describe(): è¿”å›å½“å‰æ–‡ä»¶çˆ¶æ–‡ä»¶çš„æè¿°ä¿¡æ¯(yolov5)   date_modified(): è¿”å›å½“å‰æ–‡ä»¶çš„ä¿®æ”¹æ—¥æœŸ
    # s: ä¹‹åè¦åŠ å…¥loggeræ—¥å¿—çš„æ˜¾ç¤ºä¿¡æ¯
    s = f'YOLOv5 ğŸš€ {git_describe() or file_date()} Python-{platform.python_version()} torch-{torch.__version__} '
    device = str(device).strip().lower().replace('cuda:', '').replace('none', '')  # to string, 'cuda:0' to '0'
    # å¦‚æœdeviceè¾“å…¥ä¸ºcpu  cpu=True  device.lower(): å°†deviceå­—ç¬¦ä¸²å…¨éƒ¨è½¬ä¸ºå°å†™å­—æ¯
    cpu = device == 'cpu'
    mps = device == 'mps'  # Apple Metal Performance Shaders (MPS)
    if cpu or mps:
        # å¦‚æœcpu=True å°±å¼ºåˆ¶(force)ä½¿ç”¨cpu ä»¤torch.cuda.is_available() = False
        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force torch.cuda.is_available() = False
    elif device:  # non-cpu device requested
        # å¦‚æœè¾“å…¥deviceä¸ä¸ºç©º  device=GPU  ç›´æ¥è®¾ç½® CUDA environment variable = device åŠ å…¥CUDAå¯ç”¨è®¾å¤‡
        os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable - must be before assert is_available()
        assert torch.cuda.is_available() and torch.cuda.device_count() >= len(device.replace(',', '')), \
            f"Invalid CUDA '--device {device}' requested, use '--device cpu' or pass valid CUDA device(s)"

    # è¾“å…¥deviceä¸ºç©º è‡ªè¡Œæ ¹æ®è®¡ç®—æœºæƒ…å†µé€‰æ‹©ç›¸åº”è®¾å¤‡  å…ˆçœ‹GPU æ²¡æœ‰å°±CPU
    # å¦‚æœcudaå¯ç”¨ ä¸” è¾“å…¥device != cpu åˆ™ cuda=True åæ­£cuda=False
    if not cpu and not mps and torch.cuda.is_available():  # prefer GPU if available
        # devices: å¦‚æœcudaå¯ç”¨ è¿”å›æ‰€æœ‰å¯ç”¨çš„gpuè®¾å¤‡ i.e. 0,1,6,7  å¦‚æœä¸å¯ç”¨å°±è¿”å› '0'
        devices = device.split(',') if device else '0'  # range(torch.cuda.device_count())  # i.e. 0,1,6,7
        # n: æ‰€æœ‰å¯ç”¨çš„gpuè®¾å¤‡æ•°é‡  device count
        n = len(devices)  # device count
        # æ£€æŸ¥æ˜¯å¦æœ‰gpuè®¾å¤‡ ä¸” batch_sizeæ˜¯å¦å¯ä»¥èƒ½è¢«æ˜¾å¡æ•°ç›®æ•´é™¤  check batch_size is divisible by device_count
        if n > 1 and batch_size > 0:  # check batch_size is divisible by device_count
            # å¦‚æœä¸èƒ½åˆ™å…³é—­ç¨‹åº
            assert batch_size % n == 0, f'batch-size {batch_size} not multiple of GPU count {n}'
        space = ' ' * (len(s) + 1)
        # æ»¡è¶³æ‰€æœ‰æ¡ä»¶ såŠ ä¸Šæ‰€æœ‰æ˜¾å¡çš„ä¿¡æ¯
        for i, d in enumerate(devices):
            # p: æ¯ä¸ªå¯ç”¨æ˜¾å¡çš„ç›¸å…³å±æ€§
            p = torch.cuda.get_device_properties(i)
            # æ˜¾ç¤ºä¿¡æ¯såŠ ä¸Šæ¯å¼ æ˜¾å¡çš„å±æ€§ä¿¡æ¯
            s += f"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / (1 << 20):.0f}MiB)\n"  # bytes to MB
        arg = 'cuda:0'
    elif mps and getattr(torch, 'has_mps', False) and torch.backends.mps.is_available():  # prefer MPS if available
        s += 'MPS\n'
        arg = 'mps'
    else:
        # cudaä¸å¯ç”¨æ˜¾ç¤ºä¿¡æ¯så°±åŠ ä¸ŠCPU
        s += 'CPU\n'
        arg = 'cpu'

    if not newline:
        s = s.rstrip()
    LOGGER.info(s)
    return torch.device(arg)


def time_sync():
    """è¿™ä¸ªå‡½æ•°è¢«å¹¿æ³›çš„ç”¨äºæ•´ä¸ªé¡¹ç›®çš„å„ä¸ªæ–‡ä»¶ä¸­ï¼Œåªè¦æ¶‰åŠè·å–å½“å‰æ—¶é—´çš„æ“ä½œï¼Œå°±éœ€è¦è°ƒç”¨è¿™ä¸ªå‡½æ•°
    ç²¾ç¡®è®¡ç®—å½“å‰æ—¶é—´  å¹¶è¿”å›å½“å‰æ—¶é—´
    https://blog.csdn.net/qq_23981335/article/details/105709273
    pytorch-accurate time
    å…ˆè¿›è¡Œtorch.cuda.synchronize()æ·»åŠ åŒæ­¥æ“ä½œ å†è¿”å›time.time()å½“å‰æ—¶é—´
    ä¸ºä»€ä¹ˆä¸ç›´æ¥ä½¿ç”¨time.time()å–æ—¶é—´ï¼Œè€Œè¦å…ˆæ‰§è¡ŒåŒæ­¥æ“ä½œï¼Œå†å–æ—¶é—´ï¼Ÿè¯´ä¸€ä¸‹è¿™æ ·å­åšçš„åŸå› :
       åœ¨pytorché‡Œé¢ï¼Œç¨‹åºçš„æ‰§è¡Œéƒ½æ˜¯å¼‚æ­¥çš„ã€‚
       å¦‚æœtime.time(), æµ‹è¯•çš„æ—¶é—´ä¼šå¾ˆçŸ­ï¼Œå› ä¸ºæ‰§è¡Œå®Œend=time.time()ç¨‹åºå°±é€€å‡ºäº†
       è€Œå…ˆåŠ torch.cuda.synchronize()ä¼šå…ˆåŒæ­¥cudaçš„æ“ä½œï¼Œç­‰å¾…gpuä¸Šçš„æ“ä½œéƒ½å®Œæˆäº†å†ç»§ç»­è¿è¡Œend = time.time()
       è¿™æ ·å­æµ‹è¯•æ—¶é—´ä¼šå‡†ç¡®ä¸€ç‚¹
    """
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    return time.time()


# æ²¡ç”¨åˆ°, è®¡ç®—è‡ªåŠ¨batch_sizeæ—¶ä¼šç”¨åˆ°,å‰æ³¨é‡Šå‘æˆ‘~
def profile(input, ops, n=10, device=None):
    """
    è¾“å‡ºæŸä¸ªç½‘ç»œç»“æ„(æ“ä½œops)çš„ä¸€äº›ä¿¡æ¯: æ€»å‚æ•° æµ®ç‚¹è®¡ç®—é‡ å‰å‘ä¼ æ’­æ—¶é—´ åå‘ä¼ æ’­æ—¶é—´ è¾“å…¥å˜é‡çš„shape è¾“å‡ºå˜é‡çš„shape
    :params x: è¾“å…¥tensor x
    :params ops: æ“ä½œops(æŸä¸ªç½‘ç»œç»“æ„)
    :params n: æ‰§è¡Œå¤šå°‘è½®ops
    :params device: æ‰§è¡Œè®¾å¤‡
    """
    # Usage:
    #     input = torch.randn(16, 3, 640, 640)
    #     m1 = lambda x: x * torch.sigmoid(x)
    #     m2 = nn.SiLU()
    #     profile(input, [m1, m2], n=100)  # profile over 100 iterations

    results = []
    if not isinstance(device, torch.device):
        device = select_device(device)
    print(f"{'Params':>12s}{'GFLOPs':>12s}{'GPU_mem (GB)':>14s}{'forward (ms)':>14s}{'backward (ms)':>14s}"
          f"{'input':>24s}{'output':>24s}")

    for x in input if isinstance(input, list) else [input]:
        x = x.to(device)
        # è¡¨æ˜éœ€è¦è®¡ç®—tensor xçš„æ¢¯åº¦
        x.requires_grad = True
        for m in ops if isinstance(ops, list) else [ops]:
            # ç¡®ä¿opsä¸­æ‰€æœ‰çš„æ“ä½œéƒ½æ˜¯åœ¨deviceè®¾å¤‡ä¸­è¿è¡Œ
            # hasattr(m, 'to'): åˆ¤æ–­å¯¹è±¡mæ²¡æœ‰toå±æ€§
            m = m.to(device) if hasattr(m, 'to') else m  # device
            # ç¡®ä¿æ“ä½œmå’Œtensor xæ˜¯å¤„äºç›¸åŒçš„ç²¾åº¦  é»˜è®¤xæ˜¯Float32çš„  halfå¯ä»¥å°†ç²¾åº¦å‡åŠ
            m = m.half() if hasattr(m, 'half') and isinstance(x, torch.Tensor) and x.dtype is torch.float16 else m
            # åˆå§‹åŒ–å‰å‘ä¼ æ’­æ—¶é—´dtf åå‘ä¼ æ’­æ—¶é—´dtb ä»¥åŠtå˜é‡ç”¨äºè®°å½•ä¸‰ä¸ªæ—¶åˆ»çš„æ—¶é—´(åé¢æœ‰å†™)
            tf, tb, t = 0, 0, [0, 0, 0]  # dt forward, backward
            try:
                # è®¡ç®—åœ¨è¾“å…¥ä¸ºtensor x, æ“ä½œä¸ºmæ¡ä»¶ä¸‹çš„æµ®ç‚¹è®¡ç®—é‡GFLOPs
                flops = thop.profile(m, inputs=(x,), verbose=False)[0] / 1E9 * 2  # GFLOPs
            except Exception:
                flops = 0

            try:
                for _ in range(n):
                    t[0] = time_sync()  # æ“ä½œmå‰å‘ä¼ æ’­å‰ä¸€æ—¶åˆ»çš„æ—¶é—´
                    y = m(x)  # æ“ä½œmå‰å‘ä¼ æ’­
                    t[1] = time_sync()  # æ“ä½œmå‰å‘ä¼ æ’­åä¸€æ—¶åˆ»çš„æ—¶é—´ = æ“ä½œmåå‘ä¼ æ’­å‰ä¸€æ—¶åˆ»çš„æ—¶é—´
                    try:
                        _ = (sum(yi.sum() for yi in y) if isinstance(y, list) else y).sum().backward()
                        t[2] = time_sync()
                    except Exception:  # å¦‚æœæ²¡æœ‰åå‘ä¼ æ’­
                        # print(e)  # for debug
                        t[2] = float('nan')
                    tf += (t[1] - t[0]) * 1000 / n  # æ“ä½œmå¹³å‡æ¯æ¬¡å‰å‘ä¼ æ’­æ‰€ç”¨æ—¶é—´
                    tb += (t[2] - t[1]) * 1000 / n  # æ“ä½œmå¹³å‡æ¯æ¬¡åå‘ä¼ æ’­æ‰€ç”¨æ—¶é—´
                mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0  # (GB)
                s_in, s_out = (tuple(x.shape) if isinstance(x, torch.Tensor) else 'list' for x in (x, y))  # shapes
                p = sum(x.numel() for x in m.parameters()) if isinstance(m, nn.Module) else 0  # parameters
                print(f'{p:12}{flops:12.4g}{mem:>14.3f}{tf:14.4g}{tb:14.4g}{str(s_in):>24s}{str(s_out):>24s}')
                results.append([p, flops, mem, tf, tb, s_in, s_out])
            except Exception as e:
                print(e)
                results.append(None)
            torch.cuda.empty_cache()
    return results


def is_parallel(model):
    # Returns True if model is of type DP or DDP
    return type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)


def de_parallel(model):
    # De-parallelize a model: returns single-GPU model if model is of type DP or DDP
    return model.module if is_parallel(model) else model


def initialize_weights(model):
    """åœ¨yolo.pyçš„Modelç±»ä¸­çš„initå‡½æ•°è¢«è°ƒç”¨
    ç”¨äºåˆå§‹åŒ–æ¨¡å‹æƒé‡
    """
    for m in model.modules():
        t = type(m)
        if t is nn.Conv2d:  # å¦‚æœæ˜¯äºŒç»´å·ç§¯å°±è·³è¿‡  æˆ–è€…ä½¿ç”¨ä½•å‡¯æ˜åˆå§‹åŒ–
            pass  # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif t is nn.BatchNorm2d:
            m.eps = 1e-3
            m.momentum = 0.03
        elif t in [nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU]:
            # å¦‚æœæ˜¯è¿™å‡ ç±»æ¿€æ´»å‡½æ•° inplaceæ’å€¼å°±èµ‹ä¸ºTrue
            # inplace = True æŒ‡è¿›è¡ŒåŸåœ°æ“ä½œ å¯¹äºä¸Šå±‚ç½‘ç»œä¼ é€’ä¸‹æ¥çš„tensorç›´æ¥è¿›è¡Œä¿®æ”¹ ä¸éœ€è¦å¦å¤–èµ‹å€¼å˜é‡
            # è¿™æ ·å¯ä»¥èŠ‚çœè¿ç®—å†…å­˜ï¼Œä¸ç”¨å¤šå‚¨å­˜å˜é‡
            m.inplace = True


# æ²¡ç”¨åˆ°
def find_modules(model, mclass=nn.Conv2d):
    """
    ç”¨äºæ‰¾åˆ°æ¨¡å‹modelä¸­ç±»å‹æ˜¯mclassçš„å±‚ç»“æ„çš„ç´¢å¼•  Finds layer indices matching module class 'mclass'
    :params model: æ¨¡å‹
    :params mclass: å±‚ç»“æ„ç±»å‹ é»˜è®¤nn.Conv2d
    """
    return [i for i, m in enumerate(model.module_list) if isinstance(m, mclass)]


def sparsity(model):
    """åœ¨pruneä¸­è°ƒç”¨
    ç”¨äºæ±‚æ¨¡å‹modelçš„ç¨€ç–ç¨‹åº¦sparsity   Return global model sparsity
    """
    # åˆå§‹åŒ–æ¨¡å‹çš„æ€»å‚æ•°ä¸ªæ•°a(å‰å‘+åå‘)  æ¨¡å‹å‚æ•°ä¸­å€¼ä¸º0çš„å‚æ•°ä¸ªæ•°b
    a, b = 0, 0
    # model.parameters()è¿”å›æ¨¡å‹modelçš„å‚æ•° è¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ éœ€è¦ç”¨forå¾ªç¯æˆ–è€…next()æ¥è·å–å‚æ•°
    # forå¾ªç¯å–å‡ºæ¯ä¸€å±‚çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­çš„å‚æ•°
    for p in model.parameters():
        a += p.numel()
        b += (p == 0).sum()
    # b / a å³å¯ä»¥ååº”æ¨¡å‹çš„ç¨€ç–ç¨‹åº¦
    return b / a


def prune(model, amount=0.3):
    """å¯ä»¥ç”¨äºtest.pyå’Œdetect.pyä¸­è¿›è¡Œæ¨¡å‹å‰ªæ
    å¯¹æ¨¡å‹modelè¿›è¡Œå‰ªææ“ä½œ ä»¥å¢åŠ æ¨¡å‹çš„ç¨€ç–æ€§  ä½¿ç”¨pruneå·¥å…·å°†å‚æ•°ç¨€ç–åŒ–
    https://github.com/ultralytics/yolov5/issues/304
    :params model: æ¨¡å‹
    :params amount: éšæœºè£å‰ª(æ€»å‚æ•°é‡ x amount)æ•°é‡çš„å‚æ•°
    """
    import torch.nn.utils.prune as prune  # å¯¼å…¥ç”¨äºå‰ªæçš„å·¥å…·åŒ…
    # æ¨¡å‹çš„è¿­ä»£å™¨ è¿”å›çš„æ˜¯æ‰€æœ‰æ¨¡å—çš„è¿­ä»£å™¨  åŒæ—¶äº§ç”Ÿæ¨¡å—çš„åç§°(name)ä»¥åŠæ¨¡å—æœ¬èº«(m)
    for name, m in model.named_modules():
        if isinstance(m, nn.Conv2d):
            # å¯¹å½“å‰å±‚ç»“æ„m, éšæœºè£å‰ª(æ€»å‚æ•°é‡ x amount)æ•°é‡çš„æƒé‡(weight)å‚æ•°
            prune.l1_unstructured(m, name='weight', amount=amount)  # prune
            # å½»åº•ç§»é™¤è¢«è£å‰ªçš„çš„æƒé‡å‚æ•°
            prune.remove(m, 'weight')  # make permanent
    # è¾“å‡ºæ¨¡å‹çš„ç¨€ç–åº¦ è°ƒç”¨sparsityå‡½æ•°è®¡ç®—å½“å‰æ¨¡å‹çš„ç¨€ç–åº¦
    LOGGER.info(f'Model pruned to {sparsity(model):.3g} global sparsity')


def fuse_conv_and_bn(conv, bn):
    """åœ¨yolo.pyä¸­Modelç±»çš„fuseå‡½æ•°ä¸­è°ƒç”¨
    èåˆå·ç§¯å±‚å’ŒBNå±‚(æµ‹è¯•æ¨ç†ä½¿ç”¨)   Fuse convolution and batchnorm layers
    æ–¹æ³•: å·ç§¯å±‚è¿˜æ˜¯æ­£å¸¸å®šä¹‰, ä½†æ˜¯å·ç§¯å±‚çš„å‚æ•°w,bè¦æ”¹å˜   é€šè¿‡åªæ”¹å˜å·ç§¯å‚æ•°, è¾¾åˆ°CONV+BNçš„æ•ˆæœ
          w = w_bn * w_conv   b = w_bn * b_conv + b_bn   (å¯ä»¥è¯æ˜)
    https://tehnokv.com/posts/fusing-batchnorm-and-conv/
    https://github.com/ultralytics/yolov3/issues/807
    https://zhuanlan.zhihu.com/p/94138640
    :params conv: torchæ”¯æŒçš„å·ç§¯å±‚
    :params bn: torchæ”¯æŒçš„bnå±‚
    """
    fusedconv = nn.Conv2d(conv.in_channels,
                          conv.out_channels,
                          kernel_size=conv.kernel_size,
                          stride=conv.stride,
                          padding=conv.padding,
                          dilation=conv.dilation,
                          groups=conv.groups,
                          bias=True).requires_grad_(False).to(conv.weight.device)

    # Prepare filters
    # w_conv: å·ç§¯å±‚çš„wå‚æ•° ç›´æ¥clone convçš„weightå³å¯
    w_conv = conv.weight.clone().view(conv.out_channels, -1)
    # w_bn: bnå±‚çš„wå‚æ•°(å¯ä»¥è‡ªå·±æ¨åˆ°å…¬å¼)  torch.diag: è¿”å›ä¸€ä¸ªä»¥inputä¸ºå¯¹è§’çº¿å…ƒç´ çš„2D/1D æ–¹é˜µ/å¼ é‡?
    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))
    # w = w_bn * w_conv      torch.mm: å¯¹ä¸¤ä¸ªçŸ©é˜µç›¸ä¹˜
    fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.shape))

    # Prepare spatial bias
    # b_conv: å·ç§¯å±‚çš„bå‚æ•° å¦‚æœä¸ä¸ºNoneå°±ç›´æ¥è¯»å–conv.biaså³å¯
    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias
    # b_bn: bnå±‚çš„bå‚æ•°(å¯ä»¥è‡ªå·±æ¨åˆ°å…¬å¼)
    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))
    #  b = w_bn * b_conv + b_bn   (w_bn not forgot)
    fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)

    return fusedconv


def model_info(model, verbose=False, imgsz=640):
    """ç”¨äºyolo.pyæ–‡ä»¶çš„Modelç±»çš„infoå‡½æ•°
    è¾“å‡ºæ¨¡å‹çš„æ‰€æœ‰ä¿¡æ¯ åŒ…æ‹¬: æ‰€æœ‰å±‚æ•°é‡, æ¨¡å‹æ€»å‚æ•°é‡, éœ€è¦æ±‚æ¢¯åº¦çš„æ€»å‚æ•°é‡, img_sizeå¤§å°çš„modelçš„æµ®ç‚¹è®¡ç®—é‡GFLOPs
    :params model: æ¨¡å‹
    :params verbose: æ˜¯å¦è¾“å‡ºæ¯ä¸€å±‚çš„å‚æ•°parametersçš„ç›¸å…³ä¿¡æ¯
    :params img_size: int or list  i.e. img_size=640 or img_size=[640, 320]
    """
    # n_p: æ¨¡å‹modelçš„æ€»å‚æ•°  number parameters
    n_p = sum(x.numel() for x in model.parameters())  # number parameters
    # n_g: æ¨¡å‹modelçš„å‚æ•°ä¸­éœ€è¦æ±‚æ¢¯åº¦(requires_grad=True)çš„å‚æ•°é‡  number gradients
    n_g = sum(x.numel() for x in model.parameters() if x.requires_grad)  # number gradients
    if verbose:
        # è¡¨å¤´: 'layer', 'name',  'gradient',    'parameters',    'shape',        'mu',         'sigma'
        #       ç¬¬å‡ å±‚    å±‚å   boolæ˜¯å¦éœ€è¦æ±‚æ¢¯åº¦   å½“å‰å±‚å‚æ•°é‡   å½“å‰å±‚å‚æ•°shape  å½“å‰å±‚å‚æ•°å‡å€¼    å½“å‰å±‚å‚æ•°æ–¹å·®
        print(f"{'layer':>5} {'name':>40} {'gradient':>9} {'parameters':>12} {'shape':>20} {'mu':>10} {'sigma':>10}")
        # æŒ‰è¡¨å¤´è¾“å‡ºæ¯ä¸€å±‚çš„å‚æ•°parametersçš„ç›¸å…³ä¿¡æ¯
        for i, (name, p) in enumerate(model.named_parameters()):
            name = name.replace('module_list.', '')
            print('%5g %40s %9s %12g %20s %10.3g %10.3g' %
                  (i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std()))

    try:  # FLOPs
        p = next(model.parameters())
        # stride æ¨¡å‹çš„æœ€å¤§ä¸‹é‡‡æ ·ç‡ æœ‰[8, 16, 32] æ‰€ä»¥stride=32
        stride = max(int(model.stride.max()), 32) if hasattr(model, 'stride') else 32  # max stride
        # æ¨¡æ‹Ÿä¸€æ ·è¾“å…¥å›¾ç‰‡ shape=(1, 3, 32, 32)  å…¨æ˜¯0
        im = torch.empty((1, p.shape[1], stride, stride), device=p.device)  # input image in BCHW format
        # è°ƒç”¨profileè®¡ç®—è¾“å…¥å›¾ç‰‡img=(1, 3, 32, 32)æ—¶å½“å‰æ¨¡å‹çš„æµ®ç‚¹è®¡ç®—é‡GFLOPs   stride GFLOPs
        # profileæ±‚å‡ºæ¥çš„æµ®ç‚¹è®¡ç®—é‡æ˜¯FLOPs  /1E9 => GFLOPs
        # *2æ˜¯å› ä¸ºprofileå‡½æ•°é»˜è®¤æ±‚çš„å°±æ˜¯æ¨¡å‹ä¸ºfloat64æ—¶çš„æµ®ç‚¹è®¡ç®—é‡ è€Œæˆ‘ä»¬ä¼ å…¥çš„æ¨¡å‹ä¸€èˆ¬éƒ½æ˜¯float32 æ‰€ä»¥ä¹˜ä»¥2(å¯ä»¥ç‚¹è¿›profileçœ‹ä»–å®šä¹‰çš„add_hookså‡½æ•°)
        flops = thop.profile(deepcopy(model), inputs=(im,), verbose=False)[0] / 1E9 * 2  # stride GFLOPs
        # expand  img_size -> [img_size, img_size]=[640, 640]
        imgsz = imgsz if isinstance(imgsz, list) else [imgsz, imgsz]  # expand if int/float
        # æ ¹æ®img=(1, 3, 32, 32)çš„æµ®ç‚¹è®¡ç®—é‡flopsæ¨ç®—å‡º640x640çš„å›¾ç‰‡çš„æµ®ç‚¹è®¡ç®—é‡GFLOPs
        # ä¸ç›´æ¥è®¡ç®—640x640çš„å›¾ç‰‡çš„æµ®ç‚¹è®¡ç®—é‡GFLOPså¯èƒ½æ˜¯ä¸ºäº†é«˜æ•ˆæ€§å§, è¿™æ ·ç®—å¯èƒ½é€Ÿåº¦æ›´å¿«
        fs = f', {flops * imgsz[0] / stride * imgsz[1] / stride:.1f} GFLOPs'  # 640x640 GFLOPs
    except Exception:
        fs = ''

    name = Path(model.yaml_file).stem.replace('yolov5', 'YOLOv5') if hasattr(model, 'yaml_file') else 'Model'
    LOGGER.info(f'{name} summary: {len(list(model.modules()))} layers, {n_p} parameters, {n_g} gradients{fs}')


def scale_img(img, ratio=1.0, same_shape=False, gs=32):  # img(16,3,256,416)
    """ç”¨äºyolo.pyæ–‡ä»¶ä¸­Modelç±»çš„forward_augmentå‡½æ•°ä¸­
    å®ç°å¯¹å›¾ç‰‡çš„ç¼©æ”¾æ“ä½œ
    :params img: åŸå›¾
    :params ratio: ç¼©æ”¾æ¯”ä¾‹ é»˜è®¤=1.0 åŸå›¾
    :params same_shape: ç¼©æ”¾ä¹‹åå°ºå¯¸æ˜¯å¦æ˜¯è¦æ±‚çš„å¤§å°(å¿…é¡»æ˜¯gs=32çš„å€æ•°)
    :params gs: æœ€å¤§çš„ä¸‹é‡‡æ ·ç‡ 32 æ‰€ä»¥ç¼©æ”¾åçš„å›¾ç‰‡çš„shapeå¿…é¡»æ˜¯gs=32çš„å€æ•°
    """
    # img(16,3,256,416)
    # Scales img(bs,3,y,x) by ratio constrained to gs-multiple
    if ratio == 1.0:  # å¦‚æœç¼©æ”¾æ¯”ä¾‹ratioä¸º1.0 ç›´æ¥è¿”å›åŸå›¾
        return img
        # h, w: åŸå›¾çš„é«˜å’Œå®½
    h, w = img.shape[2:]
    # s: æ”¾ç¼©åå›¾ç‰‡çš„æ–°å°ºå¯¸  new size
    s = (int(h * ratio), int(w * ratio))  # new size
    # ç›´æ¥ä½¿ç”¨torchè‡ªå¸¦çš„F.interpolate(ä¸Šé‡‡æ ·ä¸‹é‡‡æ ·å‡½æ•°)æ’å€¼å‡½æ•°è¿›è¡Œresize
    # F.interpolate: å¯ä»¥ç»™å®šsizeæˆ–è€…scale_factoræ¥è¿›è¡Œä¸Šä¸‹é‡‡æ ·
    #                mode='bilinear': åŒçº¿æ€§æ’å€¼  nearest:æœ€è¿‘é‚»
    #                align_corner: æ˜¯å¦å¯¹é½ input å’Œ output çš„è§’ç‚¹åƒç´ (corner pixels)
    img = F.interpolate(img, size=s, mode='bilinear', align_corners=False)  # resize
    if not same_shape:  # pad/crop img
        # ç¼©æ”¾ä¹‹åè¦æ˜¯å°ºå¯¸å’Œè¦æ±‚çš„å¤§å°(å¿…é¡»æ˜¯gs=32çš„å€æ•°)ä¸åŒ å†å¯¹å…¶ä¸ç›¸äº¤çš„éƒ¨åˆ†è¿›è¡Œpad
        # è€Œpadçš„å€¼å°±æ˜¯imagenetçš„mean
        # Math.ceil(): å‘ä¸Šå–æ•´  è¿™é‡Œé™¤ä»¥gså‘ä¸Šå–æ•´å†ä¹˜ä»¥gsæ˜¯ä¸ºäº†ä¿è¯hã€wéƒ½æ˜¯gsçš„å€æ•°
        h, w = (math.ceil(x * ratio / gs) * gs for x in (h, w))
        # pad img shape to gsçš„å€æ•° å¡«å……å€¼ä¸º imagenet mean
    return F.pad(img, [0, w - s[1], 0, h - s[0]], value=0.447)  # value = imagenet mean


def copy_attr(a, b, include=(), exclude=()):
    """åœ¨ModelEMAå‡½æ•°å’Œyolo.pyä¸­Modelç±»çš„autoshapeå‡½æ•°ä¸­è°ƒç”¨
    å¤åˆ¶bçš„å±æ€§(è¿™ä¸ªå±æ€§å¿…é¡»åœ¨includeä¸­è€Œä¸åœ¨excludeä¸­)ç»™a
    :params a: å¯¹è±¡a(å¾…èµ‹å€¼)
    :params b: å¯¹è±¡b(èµ‹å€¼)
    :params include: å¯ä»¥èµ‹å€¼çš„å±æ€§
    :params exclude: ä¸èƒ½èµ‹å€¼çš„å±æ€§
    """
    # Copy attributes from b to a, options to only include [...] and to exclude [...]
    # __dict__è¿”å›ä¸€ä¸ªç±»çš„å®ä¾‹çš„å±æ€§å’Œå¯¹åº”å–å€¼çš„å­—å…¸
    for k, v in b.__dict__.items():
        if (len(include) and k not in include) or k.startswith('_') or k in exclude:
            continue
        else:
            # å°†å¯¹è±¡bçš„å±æ€§kèµ‹å€¼ç»™a
            setattr(a, k, v)


def smart_optimizer(model, name='Adam', lr=0.001, momentum=0.9, decay=1e-5):
    # YOLOv5 3-param group optimizer: 0) weights with decay, 1) weights no decay, 2) biases no decay
    g = [], [], []  # optimizer parameter groups
    bn = tuple(v for k, v in nn.__dict__.items() if 'Norm' in k)  # normalization layers, i.e. BatchNorm2d()
    for v in model.modules():
        for p_name, p in v.named_parameters(recurse=0):
            if p_name == 'bias':  # bias (no decay)
                g[2].append(p)
            elif p_name == 'weight' and isinstance(v, bn):  # weight (no decay)
                g[1].append(p)
            else:
                g[0].append(p)  # weight (with decay)

    if name == 'Adam':
        optimizer = torch.optim.Adam(g[2], lr=lr, betas=(momentum, 0.999))  # adjust beta1 to momentum
    elif name == 'AdamW':
        optimizer = torch.optim.AdamW(g[2], lr=lr, betas=(momentum, 0.999), weight_decay=0.0)
    elif name == 'RMSProp':
        optimizer = torch.optim.RMSprop(g[2], lr=lr, momentum=momentum)
    elif name == 'SGD':
        optimizer = torch.optim.SGD(g[2], lr=lr, momentum=momentum, nesterov=True)
    else:
        raise NotImplementedError(f'Optimizer {name} not implemented.')

    optimizer.add_param_group({'params': g[0], 'weight_decay': decay})  # add g0 with weight_decay
    optimizer.add_param_group({'params': g[1], 'weight_decay': 0.0})  # add g1 (BatchNorm2d weights)
    LOGGER.info(f"{colorstr('optimizer:')} {type(optimizer).__name__}(lr={lr}) with parameter groups "
                f'{len(g[1])} weight(decay=0.0), {len(g[0])} weight(decay={decay}), {len(g[2])} bias')
    return optimizer


def smart_hub_load(repo='ultralytics/yolov5', model='yolov5s', **kwargs):
    # YOLOv5 torch.hub.load() wrapper with smart error/issue handling
    if check_version(torch.__version__, '1.9.1'):
        kwargs['skip_validation'] = True  # validation causes GitHub API rate limit errors
    if check_version(torch.__version__, '1.12.0'):
        kwargs['trust_repo'] = True  # argument required starting in torch 0.12
    try:
        return torch.hub.load(repo, model, **kwargs)
    except Exception:
        return torch.hub.load(repo, model, force_reload=True, **kwargs)


def smart_resume(ckpt, optimizer, ema=None, weights='yolov5s.pt', epochs=300, resume=True):
    # Resume training from a partially trained checkpoint
    best_fitness = 0.0
    start_epoch = ckpt['epoch'] + 1
    if ckpt['optimizer'] is not None:
        optimizer.load_state_dict(ckpt['optimizer'])  # optimizer
        best_fitness = ckpt['best_fitness']
    if ema and ckpt.get('ema'):
        ema.ema.load_state_dict(ckpt['ema'].float().state_dict())  # EMA
        ema.updates = ckpt['updates']
    if resume:
        assert start_epoch > 0, f'{weights} training to {epochs} epochs is finished, nothing to resume.\n' \
                                f"Start a new training without --resume, i.e. 'python train.py --weights {weights}'"
        LOGGER.info(f'Resuming training from {weights} from epoch {start_epoch} to {epochs} total epochs')
    if epochs < start_epoch:
        LOGGER.info(f"{weights} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {epochs} more epochs.")
        epochs += ckpt['epoch']  # finetune additional epochs
    return best_fitness, start_epoch, epochs


class EarlyStopping:
    # YOLOv5 simple early stopper
    def __init__(self, patience=30):
        self.best_fitness = 0.0  # i.e. mAP
        self.best_epoch = 0
        self.patience = patience or float('inf')  # epochs to wait after fitness stops improving to stop
        self.possible_stop = False  # possible stop may occur next epoch

    def __call__(self, epoch, fitness):
        if fitness >= self.best_fitness:  # >= 0 to allow for early zero-fitness stage of training
            self.best_epoch = epoch
            self.best_fitness = fitness
        delta = epoch - self.best_epoch  # epochs without improvement
        self.possible_stop = delta >= (self.patience - 1)  # possible stop may occur next epoch
        stop = delta >= self.patience  # stop training if patience exceeded
        if stop:
            LOGGER.info(f'Stopping training early as no improvement observed in last {self.patience} epochs. '
                        f'Best results observed at epoch {self.best_epoch}, best model saved as best.pt.\n'
                        f'To update EarlyStopping(patience={self.patience}) pass a new patience value, '
                        f'i.e. `python train.py --patience 300` or use `--patience 0` to disable EarlyStopping.')
        return stop


class ModelEMA:
    """ç”¨åœ¨train.pyä¸­çš„test.runï¼ˆæµ‹è¯•ï¼‰é˜¶æ®µ
    æ¨¡å‹çš„æŒ‡æ•°åŠ æƒå¹³å‡æ–¹æ³•(Model Exponential Moving Average)
    æ˜¯ä¸€ç§ç»™äºˆè¿‘æœŸæ•°æ®æ›´é«˜æƒé‡çš„å¹³å‡æ–¹æ³• åˆ©ç”¨æ»‘åŠ¨å¹³å‡çš„å‚æ•°æ¥æé«˜æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šçš„å¥å£®æ€§/é²æ£’æ€§ ä¸€èˆ¬ç”¨äºæµ‹è¯•é›†
    https://www.bilibili.com/video/BV1FT4y1E74V?p=63
    https://www.cnblogs.com/wuliytTaotao/p/9479958.html
    https://zhuanlan.zhihu.com/p/68748778
    https://zhuanlan.zhihu.com/p/32335746
    https://github.com/ultralytics/yolov5/issues/608
    https://github.com/rwightman/pytorch-image-models/blob/master/timm/utils/model_ema.py
    """

    def __init__(self, model, decay=0.9999, tau=2000, updates=0):
        """train.py
        model:
        decay: è¡°å‡å‡½æ•°å‚æ•°
               é»˜è®¤0.9999 è€ƒè™‘è¿‡å»10000æ¬¡çš„çœŸå®å€¼
        updates: emaæ›´æ–°æ¬¡æ•°
        """
        # Create EMA
        self.ema = deepcopy(de_parallel(model)).eval()  # FP32 EMA
        # if next(model.parameters()).device.type != 'cpu':
        #     self.ema.half()  # FP16 EMA
        self.updates = updates  # emaæ›´æ–°æ¬¡æ•° number of EMA updates
        # self.decay: è¡°å‡å‡½æ•° è¾“å…¥å˜é‡ä¸ºx  decay exponential ramp (to help early epochs)
        self.decay = lambda x: decay * (1 - math.exp(-x / tau))  # decay exponential ramp (to help early epochs)
        # æ‰€æœ‰å‚æ•°å–æ¶ˆè®¾ç½®æ¢¯åº¦(æµ‹è¯•  model.val)
        for p in self.ema.parameters():
            p.requires_grad_(False)

    def update(self, model):
        # æ›´æ–°emaçš„å‚æ•°  Update EMA parameters
        self.updates += 1  # emaæ›´æ–°æ¬¡æ•° + 1
        d = self.decay(self.updates)  # éšç€æ›´æ–°æ¬¡æ•° æ›´æ–°å‚æ•°è´å¡”(d)

        # msd: æ¨¡å‹é…ç½®çš„å­—å…¸ model state_dict  msdä¸­çš„æ•°æ®ä¿æŒä¸å˜ ç”¨äºè®­ç»ƒ
        msd = de_parallel(model).state_dict()  # model state_dict
        # éå†æ¨¡å‹é…ç½®å­—å…¸ å¦‚: k=linear.bias  v=[0.32, 0.25]  emaä¸­çš„æ•°æ®å‘ç”Ÿæ”¹å˜ ç”¨äºæµ‹è¯•
        for k, v in self.ema.state_dict().items():
            # è¿™é‡Œå¾—åˆ°çš„v: é¢„æµ‹å€¼
            if v.dtype.is_floating_point:  # true for FP16 and FP32
                v *= d  # å…¬å¼å·¦è¾¹  decay * shadow_variable
                # .detach() ä½¿å¯¹åº”çš„Variablesä¸ç½‘ç»œéš”å¼€è€Œä¸å‚ä¸æ¢¯åº¦æ›´æ–°
                v += (1 - d) * msd[k].detach()  # å…¬å¼å³è¾¹  (1âˆ’decay) * variable

    def update_attr(self, model, include=(), exclude=('process_group', 'reducer')):
        # è°ƒç”¨ä¸Šé¢çš„copy_attrå‡½æ•° ä»modelä¸­å¤åˆ¶ç›¸å…³å±æ€§å€¼åˆ°self.emaä¸­
        copy_attr(self.ema, model, include, exclude)
